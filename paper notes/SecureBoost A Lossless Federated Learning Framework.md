**SecureBoost: A Lossless Federated Learning Framework**  

Kewei Cheng1, Tao Fan2, Yilun Jin3, Yang Liu2, Tianjian Chen2, Qiang Yang4  

​                                                                                                                                                                                           **by Liu Yangchen**

[toc]

### Abstract  

保护用户隐私是机器学习中的一个重要问题，这一点在2018年5月欧盟（EU）推出的《通用数据保护条例》（GDPR）中得到了证明。GDPR旨在为用户提供对他们的更多控制权 个人数据，这促使我们探索具有数据共享功能而又不侵犯用户隐私的机器学习框架。 为了实现此目标，在本文中，我们提出了一种在联邦学习环境中称为SecureBoost的新型无损隐私保护树提升系统。 该联合学习系统允许使用部分相同的用户样本但具有不同功能集（对应于垂直划分的虚拟数据集）在多方上共同进行学习过程。  SecureBoost的一个优点是，它提供与非隐私保护方法相同的准确性，同时不泄露每个私有数据提供者的信息。从理论上讲，我们证明SecureBoost框架与将数据集中到一处的其他非联合梯度树增强算法一样准确。 此外，连同安全性证明一起，我们讨论了使协议完全安全所需的条件。

### Introduction  

现代社会越来越关注对我们个人数据的非法使用和利用。 在个人层面，对个人数据的不当使用可能会对用户隐私造成潜在风险。 在企业级别，数据泄漏可能会对商业利益造成严重影响。 不同社会正在采取行动。 例如，欧盟最近颁布了一项称为通用数据保护法规（GDPR）的法规。  GDPR旨在让用户更好地控制其个人数据（2016年法规; Albrecht 2016年; Mayer-Schonberger和Padova 2015年; Goodman和Flaxman 2016年）。 结果，许多严重依赖机器学习的企业开始进行彻底的改变。

尽管很难实现保护用户隐私的目标，但是在构建机器学习模型时，不同组织进行协作的需求仍然很强烈。 实际上，许多数据所有者没有足够的数据量来构建高质量的模型。 例如，零售公司具有用户交易数据，与信用评级公司一样，它们对应于不同的数据维度或特征。 同样，移动电话用户拥有其使用情况数据，但每个设备仅具有少量的用户活动数据。 为了具有用于用户偏好预测的可用模型，将有必要整合客户端收集的数据。

因此，挑战是要让两个不同的数据所有者共同协作以建立高质量的机器学习模型，同时保护用户数据的隐私和机密性。 过去，人们在交换数据时曾尝试解决用户隐私问题（Hardy et al.2017; Mohassel and Zhang 2017）。 例如，苹果公司建议使用差异隐私（Dwork，Roth等2014； Dwork 2008）来解决隐私保护问题。 差异隐私（DP）的基本思想是向数据添加经过适当校准的噪声，以便在第三方交换和分析数据时消除任何个人的歧义。 但是，正如我们在本文中所讨论的，DP仅在一定程度上防止用户数据泄漏，而不能完全排除个人身份。 此外，DP下的数据交换仍然要求组织之间进行数据交换，而GDPR这样的严格法律可能不允许这样做。 此外，DP方法在机器学习中是有损失的，因为在注入噪声后建立的模型会降低预测精度的很多性能。

最近，Google在其Android云上引入了联邦学习框架（Koneˇcn`y等，2016）。 基本思想是允许各个客户端加密其模型，然后将其上传并聚集到中央云站点。 该站点的机器学习过程可以利用这些加密的模型，而不会泄漏客户的信息。 此框架适用于数据分区框架，其中每个分区对应于从一个或多个用户收集的数据样本的子集。

在本文中，我们考虑了由多方共同设置的通用设置，以共同构建其机器学习模型，同时保护用户隐私和数据机密性。 我们的设置如图2所示。我们考虑一组参与方，每个参与方拥有自己的数据的一部分。 我们可以将位于不同参与方的数据可视化为大数据表的一个子部分，该表是通过合并不同参与方的所有数据而获得的。 然后，每一方的数据都具有以下属性：

1.将大数据表垂直拆分，以使数据在各方之间按特征维度拆分；

2.只有一个数据提供者具有标签信息；

3.用户在不同参与方之间有部分重叠。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkfigure1.PNG)

-----------------------------------------------------------

**Figure 1:** Illustration of the proposed SecureBoost framework  

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkfigure2.PNG)

-----------------------------------

**Figure 2:** Vertically partitioned data set  

然后，我们的目标是允许各方为某个指定标签建立预测模型，同时禁止任何一方获取有关其他方数据的任何信息。

我们的上述设置有几个优点。 与大多数现有的隐私保护数据挖掘和机器学习工作相比，我们设置的复杂性大大增加了。 与水平分割数据的情况不同，上述设置需要一种更复杂的机制来分解每一方的损失函数（Vaidya 2008; Vaidya and Clifton 2005; Hardy et al.2017）。另外，在各方的每个模型构建过程中，只有一个数据提供者拥有标签信息。 它要求我们提出一种安全的协议来指导学习过程，而不是在各方之间明确共享标签信息。 最后，数据机密性和隐私问题阻止了当事方在建立模型时暴露自己的用户，这些用户在组中并不常见。 因此，实体对齐也应以足够安全的方式进行。

在本文中，我们提出了一种新颖的端到端隐私保护树提升算法和称为SecureBoost的框架，以实现联合设置中的机器学习。 与以前的联合学习框架不同，该框架在用户维度上拆分数据，而当在特征维度上在不同各方之间拆分数据时，我们的框架可确保完成协作模型的构建。 我们的联合学习框架分两个步骤运行。 首先，我们在隐私保护约束下找到双方之间的共同用户。 然后，我们可以协作学习共享的分类或回归模型，而不会彼此泄漏任何用户信息。 我们将主要贡献总结如下：

- 我们正式定义了在联合学习的环境中对垂直划分的数据进行隐私保护的机器学习的新问题。
- 我们提出了一种方法，可以为每个参与方共同训练高质量的tree boosting  模型，同时将训练数据在多个参与方中保密。 我们在没有受信任的第三方参与的情况下进行了机器学习过程。
- 最后并且重要的是，我们证明了我们的方法是无损的，就其意义而言，它与将所有数据都集中到一个中央位置的任何集中式非隐私保护方法一样准确。
- 此外，连同安全性证明一起，我们讨论了使协议完全安全所需的条件。

### Preliminaries and Related Work  

关于隐私保护机器学习的现有文献广泛地解决了两个目标：用于学习模型或作为现有模型的输入的数据的隐私。 为了保护用于学习模型的数据的私密性（Shokri and Shmatikov 2015; Abadi et al.2016），作者建议利用差分隐私来学习深度学习模型。 作为最流行的隐私保护技术之一，差分隐私（Dwork 2008）通过将噪声注入原始数据集来保护敏感数据，从而最大程度地减少了从单个记录中泄漏的信息量。 即使差异性隐私确保了识别个人记录的可能性非常低，但仍有泄漏的可能性，这与GDPR的要求背道而驰。 为了解决上述问题，Google引入了联邦学习框架，将模型训练带到每个移动终端（Koneˇcn`y等人，2016年）。 它通过禁止数据转出来实现隐私保护的目标。 另一种隐私保护技术是着重于推理阶段而不是训练阶段。 微软提出了一种基于同态加密的加密深度学习框架CryptoNets（Gilad-Bachrach et al.2016），以使受过训练的神经网络能够对加密数据进行加密预测。 但是，必须牺牲准确性来获得安全性。 在（Rouhani，Riazi和Koushanfar，2017年）中，提出了另一个框架DeepSecure，该框架使用Yao的Garbled Circuit（GC）协议安全地对加密数据进行深度学习执行。 尽管它不涉及实用程序和隐私之间的折衷，但是它效率低下。

所有上述方法都是针对其数据提供者为不同实体记录相同功能的水平分区数据而设计的。 我们考虑如图2所示的垂直数据分区，其中多方在不同站点记录不同的功能。 不同于水平分割（假设整体发生在数据样本上），水平分割与普通用户建立模型。 如何协同建立模型是一个悬而未决的问题。 先前的一些工作讨论了在垂直分区的数据上保留隐私的决策树（Vaidya和Clifton 2005； Vaidya等人2008）。但是，他们提出的方法必须揭示给定属性上的类分布，这将导致潜在的安全风险。 此外，它们只能处理离散数据，这对于实际情况而言不太实用。相比之下，我们的方法可确保对数据提供更安全的保护，并且可以轻松地应用于连续数据。 在（Djatmiko等人，2017）中，Patrini等人。 提出了一个框架，该框架通过使用泰勒展开近似非线性逻辑损失来对加密的垂直分区数据共同执行逻辑回归。 显然，在这种近似中，该算法将不可避免地导致精度损失。相反，我们提出了一种本质上无损的新颖方法。 我们相信，SecureBoost框架是在垂直分区的数据上实现隐私保护的联邦学习的首次尝试，该数据在准确性和安全性之间取得了平衡。

### Problem Statement  

现在，我们正式定义我们的问题，并阐明我们的工作环境与以前的作品之间的区别。 令$\left\{\mathbf{X}^{k} \in \mathbb{R}^{n_{k} \times d_{k}}\right\}_{k=1}^{m}$是分布在$m$个私有方上的数据矩阵，每行$\mathbf{X}^{k}_{i*} \in \mathbb{R}^{1 \times d_{k}}$是数据实例。 我们使用$\mathcal{F}^{k}=\left\{f_{1}, \ldots, f_{d_{k}}\right\}$表示相应数据矩阵$\mathbf{X}^{k} $特征集。 如果我们认为所有数据都来自包含所有用户和所有功能的虚拟大数据表，那么我们可以将数据视为从大型虚拟表中垂直拆分而来，跨不同的参与者，从而使每一方都拥有不同的垂直分区数据集 在一部分用户上。 两方$p$和$q$具有不同的特征集，表示为$\mathcal{F}^{p} \cap \mathcal{F}^{q}=\varnothing, \forall p \neq q \in\{1 \ldots m\}$。 不同的数据提供者也可能拥有不同的用户集，从而允许一定程度的重叠。 也就是说，站点$n_1...n_m$的各方可能彼此不同。 如前所述，在为常见任务构建模型时，我们认为只有一个数据提供者具有用于分类或回归的类属性。 我们将类别标签表示为$\mathbf{y}^{k}_{i*} \in \mathbb{R}^{n_{k} \times 1}$，其中类别标签由第$k$方持有。

**Definition 1. Active Party:  **

我们将主动方定义为同时拥有数据矩阵和类标签的数据提供者。
由于类别标签信息对于监督学习是必不可少的，因此必须有一个活跃的参与者可以访问标签$y$。 活跃的参与者自然会在联合学习中担当主导服务器的角色。

**Definition 2. Passive Party:  **

我们将仅具有数据矩阵的数据提供者定义为被动方。
被动方在联邦学习环境中扮演客户的角色。 他们还需要建立一个模型来预测类标签$y$，以用于其预测目的。 因此，他们必须与主动方合作以建立他们的模型，以使用自己的功能为未来的用户预测$y$。
联邦学习中对垂直划分的数据进行隐私保护的机器学习问题可以表述为： 

**Given:  **垂直划分的数据矩阵$\left\{\mathbf{X}^{k} \right\}_{k=1}^{m}$分布在$m$个私人参与方上，类别标签$y$分布在主动方上。

**Learn:  **机器学习模型$M$，不会在此过程中将任何一方的数据矩阵的信息提供给其他方。 模型$M$是在每一方$i$处具有投影$M_i$的函数，使得$M_i$接受其自身特征$X_i$的输入。

**Lossless Constraint:   **我们要求模型$M$是无损的，这意味着在训练数据的联邦学习下，$M$的损失与当$M'$建立在所有数据的并集上时，$M'$的损失相同。

### Federated Learning with SecureBoost  

作为最广泛使用的机器学习算法之一，梯度树提升模型（Friedman等人2000）在许多机器学习任务中表现出色，例如欺诈检测（Oentaryo等人2014），特征选择（Li等人。  2017）和产品推荐（He et al.2014）。 在本节中，我们提出一种新颖的梯度树增强算法，在联邦学习的设置中称为SecureBoost。 如图1所示，SecureBoost包含两个主要步骤。 首先，它在隐私约束下对齐数据。 其次，它可以协作学习共享的梯度树提升模型，同时将所有训练数据在多个私人参与方中保密。下面，我们依次解释每个部分。

我们的首要目标是在所有参与方之间找到一组通用的数据样本，以建立联合模型$M$。当数据在多个参与方之间进行垂直分区时，不同参与方拥有不同但部分重叠的用户。 这些用户可以通过其唯一的用户ID进行标识。 一个问题是如何在各方之间找到共同的共享用户或数据样本而又不损害用户集的非共享部分。 特别是，我们通过使用数据库间交叉点的隐私保护协议在加密方案下对齐数据样本（Liang和Chawathe 2004）。

在隐私约束下跨不同方调整数据之后，我们现在考虑在多方联合构建树集成模型而又不违反联邦学习中的隐私的问题。 在进一步讨论算法细节之前，我们首先介绍联合学习的一般框架。 在联邦学习中，典型的交互包括四个步骤。 首先，每个客户端都从服务器下载当前的全局模型。 接下来，每个客户端都基于其本地数据和当前全局模型（位于活动方中）计算更新后的模型。 第三，每个客户端都将加密后的模型更新发送回服务器。 最后，服务器汇总这些模型更新并构建改进的全局模型。

遵循联邦学习的一般框架，我们可以看到，要在联邦学习的环境中实现保护隐私的树提升框架，实质上，我们必须回答以下三个问题：（1）每个客户如何（即， 被动方）根据其本地数据计算更新的模型，而无需参考类别标签？  （2）服务器（即主动方）如何汇总所有更新的模型并获得新的全局模型？（3）如何在各方之间共享更新的全局模型而又不会在推理时泄漏任何信息？ 为了回答这三个问题，我们首先在非联合设置中回顾了树集成模型XGBoost（Chen和Guestrin 2016）。

给定具有$n$个样本和$d$个特征的数据集$\mathbf{X}\in \mathbb{R}^{n \times d}$，XGBoost通过使用$K$个回归树来预测输出。
$$
\hat{y}_{i}=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right)
\tag{1}
$$
为了学习等式（1）中使用的回归树模型集，它在第$t$次迭代中贪婪地添加树$f_t$，以最大程度地减少以下损失。
$$
\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)
\tag{2}
$$
其中 $\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}, g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right)$，$h_{i}=\partial_{\hat{y}^{(t-1)}}^{2} l\left(y_{i}, \hat{y}^{(t-1)}\right)$

在第$t$次迭代中构造回归树时，它从深度为0的树开始，并为树的每个叶节点添加一个拆分，直到达到最大深度。 特别是，它采用以下公式确定最佳分割。
$$
\mathcal{L}_{\text {split }}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
\tag{3}
$$
在上式中，$I_L$和$I_R$是拆分后左右树节点的实例空间。 将得分最高的拆分选择为最佳拆分。

当获得最佳树形结构时，叶$j$的最佳权重$w_j ^*$可通过以下公式计算：
$$
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
\tag{4}
$$
其中$I_j$是叶子$j$的实例空间。

通过以上审查，我们得出以下结论：（1）候选分割的评估和最佳叶权重的计算仅取决于$g_i$和$h_i$。
   （2）计算$g_i$和$h_i$需要类标签。 例如，当我们将逻辑损失作为损失函数时，我们有$g_{i}=-y_{i}\left(1-\frac{1}{1+e^{-y_{i}^{(t-1)}}}\right)+\left(1-y_{i}\right)\frac{1}{1+e^{-y_{i}^{(t-1)}}}$和$h_{i}=\frac{e^{-y_{i}^{(t-1)}}}{\left(1+e^{\left.-y_{i}^{(t-1)}\right.}\right)^{2}}$。 因此，一旦获得$y_i^{（t-1）}$的值，就很容易从$g_i$和$h_i$中恢复类标签。

在上述观察的指导下，我们现在讨论如何使非联合梯度提升树模型适应联邦学习环境。 根据观察（1），我们可以看到每个被动方一旦获得$g_i$和$h_i$，就可以仅凭其本地数据独立确定局部最优分割。 因此，幼稚的解决方案要求主动方向每个被动方发送$g_i$和$h_i$。但是，根据观察结果（2），$g_i$和$h_i$也应被视为敏感数据，因为它们可用于发现类标签信息。 为了确保安全，被动方无法直接访问$g_i$和$h_i$。 为了使$g_i$和$h_i$保持机密，我们要求主动方将$g_i$和$h_i$加密后再将它们发送给被动方。 剩下的挑战是如何为每个被动方确定具有加密$g_i$和$h_i$的局部最优分割。

根据等式（5），如果可以为每个可能的分割计算出$g_l=\sum_{i \in I_{L}} g_{i}$和$h_l=\sum_{i \in I_{L}} h_{i}$，则可以找到最佳分割，其中$I_L$是分割之后左节点的实例空间。 接下来，我们展示如何使用==加法同态加密方案==获得带有加密的$g_i$和$h_i$，即$g_l$和$h_l$（Paillier 1999）。

首先，我们在加法同态加密方案下将数字$u$的加密定义为$\langle u\rangle$。 回顾加同态加密方案的主要属性，对于任意两个数字$u$和$v$，我们有$\langle u\rangle+\langle v\rangle=\langle u+v\rangle$。 因此，$\langle h_l\rangle$等于$\sum_{i \in I_{L}} \langle h_{i}\rangle$，类似地，$\langle g_l\rangle$可以通过$\sum_{i \in I_{L}} \langle g_{i}\rangle$计算。 通过利用加法同态加密方案，可以按以下方式找到最佳分割。 首先，每个被动方为本地所有可能的拆分计算$\langle g_l\rangle$和$\langle h_l\rangle$。 然后，它将值发送回主动方。 在从所有被动方收集值之后，主动方对$\langle g_l\rangle$和$\langle h_l\rangle$进行解密，并根据等式（5）计算全局最优分割。 在这种情况下，主动方和每个被动方之间的通信成本为单次拆分为$2 * n * d * ct$。 此处，$ct$表示密文的大小，$n$表示与要拆分的节点关联的实例数，$d$表示被动方保留的特征数。

我们可以观察到该解决方案效率不高，因为它要求将$\langle g_l\rangle$和$\langle h_l\rangle$转移给所有可能的拆分候选对象。 为了构建通信成本较低的树，我们利用（Chen and Guestrin 2016）提出的近似框架，其详细计算如算法1所示。对于每个被动方，它映射而不是直接计算$\langle g_l\rangle$和$\langle h_l\rangle$将特征放入存储桶中，然后基于存储桶聚合加密的渐变统计信息。 这样，主动方只需要从所有被动方收集汇总的加密梯度统计信息。 结果，它可以如算法2中所述确定全局最佳拆分。在这种情况下，用于构建回归树的通信成本可以降低到$2 *（n / q）*d * ct$，其中$q$表示一个存储桶中的实例数。 显然，我们有$（1 / q）<<1$。因此，我们确实可以降低通信成本。 主动方获得全局最优拆分后，[party id (i), feature id (k), threshold id(v)  ]，它将返回特征id $k$和阈值id $v$到相应的被动方$i$。 被动方$i$根据$k$和$v$的值确定所选属性的值。然后，它根据所选属性的值对当前实例空间进行分区。 此外，它会在本地构建一个查找表来记录所选属性的值[功能，阈值]，如图3所示。然后，它返回记录的索引和分割后左节点的实例空间$（I_L）$返回主动方。 主动方根据接收到的实例空间拆分当前节点，并将当前节点与[party id，record id]关联，直到达到停止标准或最大深度。 所有叶子节点都存储在主动方内部。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkAlgorithm1.PNG)

-------------------

**Algorithm 1** Aggregate Encrypted Gradient Statistics  

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkAlgorithm2.PNG)

---------------------

**Algorithm 2** Split Finding  

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkfigure3.PNG)

---------------------

**Figure 3:** An illustration of prediction.  

#### Federated Inference based on the Learned Model  

在本节中，我们将描述如何使用学习的模型（在各方之间分配）对新实例进行分类，即使要分类的实例的特征是私有的并且在各方之间分配的。 由于每个站点都知道自己的功能（因此可以评估分支），但对其他站点一无所知，因此，我们需要一个安全的分布式协议来根据决策来控制站点之间的传递。

为了说明推理过程，我们考虑一个具有3个参与方的系统，如图3所示。具体地说，参与方1是主动参与方，它收集有关用户每月账单支付和教育程度的信息以及标签信息。 第2方和第3方是被动方，分别具有[年龄，性别，婚姻状况]和[给定量]的特征。 假设我们希望知道用户X6是否会按时付款。 所有站点都必须合作进行预测。 整个过程由主动方协调。 从根开始，通过参考记录$[partyid：1，record id：1]$，主动参与者知道参与者1拥有根节点。 因此，它要求方1根据记录ID 1从其查找表中检索相应的属性Bill Payment。由于分类属性是Bill Payment，并且方1知道用户X6的帐单支付是4367，小于阈值5000，因此它决定将其下移到其左子节点1。 引用与节点1关联的记录$[party id:3, record id:1]$，并要求参与者3进行相同的操作。这个过程一直持续到到达叶子为止。

#### Theoretical Assessment for Lossless Property  

**Theorem 1.**   *只要模型$M$和$M'$具有相同的初始化和参数，SecureBoost将是无损的，如部分问题陈述中所定义。*

证明：在联邦学习的设置下，模型$M$的损失与基于所有数据的并集建立$M'$时的$M'$损失相同，因为$M'$和$M'$相同。 根据等式（5），$g_i$和$h_i$是计算最佳分割所需的唯一信息。 假设具有相同的初始化，每次迭代，在两种设置下每个实例具有相同的$g_i$和$h_i$值，则模型$M$和$M'$在树的构造过程中始终可以实现相同的最佳分割。 因此，$M'$和$M'$是相同的，这确保了无损的特性。

#### Security Discussion  

在本节中，我们讨论建议的SecureBoost框架的安全性。 特别是，我们将提供对框架信息泄漏的详细分析，并在存在半诚实对手的情况下讨论我们框架的安全性。 此外，连同安全性证明一起，我们讨论了使协议完全安全所需的条件。

#### Analysis of Information Leakage  

由于SecureBoost由两个组件组成，因此我们分别讨论这两个组件的信息泄漏。

在隐私保护的实体对齐过程中，加密技术可确保只有各方共享共同用户的ID才能透露任何信息。 尽管揭示普通共享用户的ID可能会引起一些潜在的风险，但是在大多数情况下，这些泄漏级别是可以接受的。

对于树集成模型的构建，所揭示的全部内容包括：（1）各方知道每个拆分的实例空间；  （2）各方都知道自己拥有的树节点；  （3）主动方知道每个被动方拥有的特征数量；  （4）主动方知道$g_l$和$h_l$的实际值；  （5）主动方知道哪个站点负责每个节点的决策。 考虑具有一个被动方和一个主动方的系统，我们现在讨论由泄漏的信息引起的潜在安全风险。

首先，我们研究被动方可以从主动方中学到多少信息。 众所周知，SecureBoost本质上是一个决策树模型。 尽管其叶节点不持有类标签，但与同一叶关联的实例仍强烈表明它们可能属于同一类或导致相似的回归结果。 因此，在SecureBoost中，我们==要求被动方不知道叶节点，以防止标签信息泄露==。 但是，这种保护不足以保证安全性。 让我们考虑一个被动方拥有两个叶节点的父节点的情况。 在这种情况下，这些叶节点的实例空间不再对被动方隐藏。 被动方可以猜测与同一叶子关联的所有实例都属于同一类。 推论的置信度由叶纯度确定，其中叶纯度是指属于多数类别的样品的比例。 因此，我们以叶片纯度为指标，对SecureBoost进行定量信息泄漏分析。 更准确地说，我们考虑二进制分类的情况，原因是它可能引起最大的安全风险。

根据等式（2），要学习SecureBoost模型，我们在第$t$次迭代中贪婪地添加决策树$f_t$以拟合残差$y_{i}-\hat{y}_{i}^{(t-1)}$。 因此，当t> 1时，与同一叶关联的实例仅表明它们可能具有相似的残差，而这些残差不能直接用于推断标签信息。 但是，当t = 1时，$f_1$尝试拟合标签$y_i$。 在这种情况下，叶节点的实例空间可以显示标签信息。 ==因此，我们对安全性的关注主要集中在我们可以从第一棵树$f_1$推断出多少信息上==。 让我们从定理2开始分析。

**Theorem 2.**   *对于学习的SecureBoost模型，信息泄漏是由第一棵树的叶子的重量决定的。*

证明： 二进制分类问题的损失函数如下所示。
$$
L=y_{i} \log \left(1+e^{-\hat{y}_{i}}\right)+\left(1-y_{i}\right) \log \left(1+e^{\hat{y}_{i}}\right)
\tag{5}
$$
基于损失函数，在第一次迭代的决策树构建过程中，我们有$g_{i}=\hat{y}_{i}^{(0)}-y_i $和$h_{i}=\hat{y}_{i}^{(0)} *\left(1-\hat{y}_{i}^{(0)}\right)$。 具体地说，将$\hat{y}_{i}^{(0)}$作为初始化值。 假设我们将所有$\hat{y}_{i}^{(0)}$初始化为0 <a <1。根据等式（4），对于与特定叶$j$相关的实例，$\hat{y}_{i}^{(1)}=S\left(w_{j}^{*}\right)=S\left(-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}\right)$其中$S（x）$是sigmoid  函数。 假设与叶$j$相关的实例数为$n_j$，正样本的百分比为$θ_j$。 当$n_j$较大时，我们可以忽略$λ$。 因此，我们有$w_i^*=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}}=-\frac{\theta * n *(a-1)+(1-\theta) * n * a}{n * a *(1-a)}=-\frac{\theta * n *(a-1)+(1-\theta) * n * a}{n * a *(1-a)}=\frac{a-\theta}{a(a-1)}$注意$max（θ，1-θ）$是叶子$j$的叶子净化 。 换句话说，给定学习型SecureBoost模型，可以从第一棵树的叶子的重量推断出信息泄漏。

==根据定理2，只要第一棵树的叶子的重量足够接近==$S\left(\frac{2 a-1}{2 a(a-1)}\right)$，该协议就被认为是安全的。

其次，我们关注主动方是否可以了解被动方的私人信息。 具体来说，我们担心安全性，如果主动方可以一定程度地恢复被动方拥有的部分功能。 在训练期间，主动方学习（1）每个拆分的实例空间；  （2）自身持有的树节点；  （3）每个被动方拥有的特征数量；  （4）$g_l$和$h_l$的实际值；  （5）哪个站点负责每个节点的决策。 为了恢复特征，主动方必须学习与特定特征有关的所有实例之间的部分顺序关系。 但是，它知道的唯一信息是如何最好地分割实例空间，这显然不足以了解偏序关系。

一般来说，根据我们的分析，SecureBoost的信息泄漏级别是可以接受的。

#### Semi-Honest Security  

在本小节中，我们将讨论半诚实假设下我们框架的安全性。 在我们的安全定义中，所有各方都是诚实但好奇的。 一些腐败方可能会相互合作以收集私人信息。 具体来说，我们要求主动方不得与任何被动方串通。 现在，我们证明Secureboost在安全性定义下是安全的。

证明：我们的SecureBoost系统可以分为两部分，第一部分仅包括主动方，第二部分包括所有被动方。 当所有被动方合谋时，该系统等于具有一个主动方和一个==超级被动方==的系统。 这个超级被动方拥有被动方的所有功能。 如“信息泄漏分析”一节所述，我们已经证明，当系统只有一个主动方和一个被动方时，信息泄漏的程度是可以接受的。 因此，在半诚实的假设下，我们的系统是安全的。

#### Completely SecureBoost  

如“信息泄漏分析”部分所述，我们的主要安全隐患是叶节点的实例空间可能会泄露过多信息，而被动方与主动节点共同构建树集成模型时，确实有机会知道叶节点的实例空间。 为了缓解此问题，我们提出了完全安全启动（Securely SecureBoost）以防止被动方构造第一棵树。 ==与SecureBoost不同，完全SecureBoost的主动方根据自己的功能独立学习第一棵树，而不是与被动方协作。 从而，可以保护第一棵树的叶节点的实例空间。== 在这种情况下，被动方只能学习残差。 尽管我们直观地说明，一旦第一棵树受到保护，残差就不会透露太多信息，但是为了使它看起来更合理，我们现在给出定理3中给出的理论证明。

**Theorem 3.  ** *当前一棵树的叶子纯度很高时，树的残差不会显示很多信息。*

证明：如前所述，对于二元分类问题，我们有$g_{i}=\hat{y}_{i}^{(t-1)}-y_i $和$h_{i}=\hat{y}_{i}^{(t-1)} *\left(1-\hat{y}_{i}^{(t-1)}\right)$，其中$g_i∈[−1  ，1]$。 因此，
$$
\begin{array}{l}
\text { if } y_{i}=0, h_{i}=g_{i}\left(1-g_{i}\right) \\
\text { if } y_{i}=1, h_{i}=-g_{i}\left(g_{i}+1\right)
\end{array}
\tag{6}
$$
当我们在第$t$次迭代中使用$k$个叶子构造决策树以适合前一棵树的残差时，本质上，我们将数据分为$k$个簇以最小化后续损失。
$$
\begin{aligned}
L &=-\sum_{j=1}^{k} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}} \\
&=-\sum_{j=1}^{k} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}^{N}} g_{i}\left(1-g_{i}\right)+\sum_{i \in I_{j}^{P}}-g_{i}\left(1+g_{i}\right)}
\end{aligned}
\tag{7}
$$
我们知道$\hat{y}_{i}^{(t-1)}∈[0，1]$和$g_{i}=\hat{y}_{i}^{(t-1)}-y_i $。 因此，对于正样本，我们有$g_i∈[−1，0]$，对于负样本，我们有$g_i∈[0，1]$。 考虑到$g_i$的范围，我们将上述等式重写如下。
$$
\sum_{j=1}^{k} \frac{\left(\sum_{i \in I_{j}^{N}}\left|g_{i}\right|-\sum_{i \in I_{j}^{P}}\left|g_{i}\right|\right)^{2}}{\sum_{i \in I_{j}^{N}}\left|g_{i}\right|\left(\left|g_{i}\right|-1\right)+\sum_{i \in I_{j}^{P}}\left|g_{i}\right|\left(\left|g_{i}\right|-1\right)}
\tag{8}
$$
其中，$I_j^N$和$I_j^P$分别表示与叶子$j$相关的负样本和正样本的集合。我们把$| g_i |$中正样本的期望表示为$\mu_{p}$ ，把$| g_i |$中负样本的期望表示为$\mu_{n}$?。 当我们有大量的样本但离开节点$k$的数目较少时，我们可以使用以下方程式来近似式（8）。
$$
\sum_{j=1}^{k} \frac{\left(n_{j}^{n} \mu_{n}-n_{j}^{p} \mu_{p}\right)^{2}}{n_{j}^{n} \mu_{n}\left(\mu_{n}-1\right)+n_{j}^{p} \mu_{p}\left(\mu_{p}-1\right)}
\tag{9}
$$
其中$n_j^n$和$n_j^p$代表与叶子$j$相关的负样本和正样本的数量。 由于$\mu_n∈[0，1]$和$\mu_p∈[0，1]$，我们知道分子必须为正，分母必须为负。 因此，整个方程必须为负。 最小化等式（9）等于最大化分子，同时最小化分母。 注意，分母为$\sum x^{2}$，分子为$\left(\sum x\right)^{2}$，其中$x∈[0，1]$。
   该方程以分子为主导。 因此，==将等式（9）最小化可以看作是分子==$\left(n_{j}^{n} \mu_{n}-n_{j}^{p} \mu_{p}\right)^{2}$==的最大化==。 理想情况下，我们需要$n_j^n$ = $n_j^p$，以防止标签信息泄露。 当$| \mu_n − \mu_p | $越大，越有可能实现目标。 而且我们知道对于负样本$\left|g_{i}\right|=\left|\hat{y}_{i}^{(t-1)}-y_{i}\right|=\hat{y}_{i}^{(t-1)}$和对于正样本$\left|g_{i}\right|=\left|\hat{y}_{i}^{(t-1)}-y_{i}\right|=1-\hat{y}_{i}^{(t-1)}$。 由此，$\mu_{n}=\frac{1}{N_{n}} \sum_{j=1}^{k} \left(1-\theta_{j}\right) n_{j}\hat{y}_{i}^{(t-1)}$，$\mu_{p}=\frac{1}{N_{p}} \sum_{j=1}^{k} \theta_{j} n_{j}\left(1-\hat{y}_{i}^{(t-1)}\right)$。  $|\mu_n-\mu_{p}|$可以计算如下。
$$
\begin{array}{l}
\left|\mu_{n}-\mu_{p}\right| \\
=\left|\frac{1}{N_{n}} \sum_{j=1}^{k}\left(1-\theta_{j}\right) n_{j} \hat{y}_{i}^{(t-1)}-\frac{1}{N_{p}} \sum_{j=1}^{k} \theta_{j} n_{j}\left(1-\hat{y}_{i}^{(t-1)}\right)\right|
\end{array}
\tag{10}
$$
其中，$N_n$和$N_p$对应于阴性样本和阳性样本的总数。  $θ_j$是在第$（t-1）$次迭代（先前的决策树）上与决策树的叶子$j$相关的正样本的百分比。  $n_j$表示与先前决策树的叶子$j$相关联的实例数。$\hat{y}_{i}^{(t-1)}=S\left(w_{j}\right)$其中，$w_j$表示前一决策树的第$j$个叶子的权重。 当正样本和负样本平衡时，$N_n$ = $N_p$，我们有
$$
\begin{aligned}
&\left|\mu_{n}-\mu_{p}\right| \\
=& \frac{1}{N_{n}} \mid \sum_{j=1}^{k}\left(\left(1-\theta_{j}\right) n_{j} S\left(w_{j}\right)-\theta_{j} n_{j}\left(1-S\left(w_{j}\right)\right) \mid\right.\\
=& \frac{1}{N_{n}} \sum_{j=1}^{k} n_{j}\left|\left(S\left(w_{j}\right)-\theta_{j}\right)\right| \\
=& \frac{1}{N_{n}} \sum_{j=1}^{k} n_{j}\left|\left(S\left(\frac{a-\theta_{j}}{a(a-1)}\right)-\theta_{j}\right)\right|
\end{aligned}
\tag{11}
$$
从式（11）可以看出，当$S\left(\frac{a-\theta_{j}}{a(a-1)}\right)=a$时，达到最小值。 通过求解方程，我们得到了$θ_j$的最优解，即$\left.\theta_{j} *=a\left(1+(1-a) \ln \left(\frac{a}{1-a}\right)\right)\right)$。为了获得更大的$\mu_n-\mu_p$，我们希望从$θ_j$到$θ_j∗$的偏差尽可能大。 当我们对$a$进行适当的初始化时，例如$a = 0.5$，则$θ_j∗ = 0.5$。 在这种情况下，最大化$|θ_j−θ_j∗ |$ 与最大化$max（θ_j，1-θ_j）$相同，这正是叶片纯度。 因此，我们证明了高的叶片纯度将保证$\mu_n$和$\mu_p$之间的差异很大，从而最终减少了信息泄漏。 我们完成了证明。

==给定定理3，我们可以得出结论，当第一棵树学习到足以用残差掩盖实际标签的信息时，完全SecureBoost是安全的。==

### Experiments  

在本节中，我们将对两个公共数据集进行实验。 这些数据集的摘要如下所示。

**Credit 1：**涉及分类用户是否会遭受严重财务问题的问题。 它总共包含150000个实例和10个属性。

**Credit 2：**它也是一个信用评分数据集，与预测用户是否按时付款的任务相关。 它包含30000个实例和总共25个属性。

在我们的实验中，我们将2/3的数据集用于训练，将其余的数据集用于测试。 我们将数据垂直拆分为两半，然后将其分发给两个参与者。 为了公平地比较不同的方法，我们将所有回归树的最大深度设置为3，将用于拟合各个回归树的样本比例设置为0.8，将学习率设置为0.3。  Paillier加密方案被视为我们的加法同构方案，密钥大小为512位。 所有实验均在配备8GB RAM和Intel Core i5-7200u CPU的计算机上进行。

#### Scalability  

由于SecureBoost由两个组件组成，即隐私保护实体对齐和安全的联合树增强系统，因此我们分别研究每个组件的可伸缩性。

**Efficiency of Privacy-Preserving Entity Alignment  **在评估隐私保护实体对齐算法的可伸缩性时，我们考虑只有两方的系统。分配给A和B的样本数量是要考虑的重要因素。 为了研究两个因素的影响，我们分别将A和B在对数刻度上分配的样本数量从1K更改为1M。我们通过修复另一个变化来研究每个变化的影响，以研究变化如何影响运行时间。 结果示于表1，具有以下观察结果。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworktable1.PNG)

----------------------------------

**Table 1:** Runtime for Entity Alignment  

- 通常，运行时变化为 A分配的样本大小与B分配的样本大小有相似的趋势，这表明A和B配的样本数分别对运行时间有同样的影响。
- 运行时间在很大程度上取决于max（#samples A，#samples B）。 当在A上分配的样本大小等于在B上分配的样本大小时，运行时间几乎随样本大小的增加线性增加。
- 当在双方A和B上分发的样本数均为1M时，对齐实体只需要大约16分钟的计算时间，这是相当有效的。该观察结果验证了我们的实体对齐算法的可伸缩性。

**Efficiency of Secure Federated Tree Boosting System**  我们注意到，安全的联合树增强系统的有效性可能受到（1）收敛速度的影响；  （2）个体回归树的最大深度；  （3）数据集的样本量；  （4）数据集的特征量。 在本小节中，我们分别研究所有四个变量对学习时间的影响。 所有实验均在数据集Credit 2上进行。

首先，我们对我们提出的系统的收敛速度感兴趣。 我们将SecureBoost与非联合树提升实施（包括GBDT和XGBoost）的收敛速度进行了比较。 从图4可以看出，SecureBoost在训练数据集上显示了与其他非联合基线方法相似的学习曲线。 它在测试数据集上的性能比其他性能稍好。 另外，我们可以看到，随着增强阶段的增加，训练损失和测试损失都开始迅速下降。 当增强阶段从10增加到25时，训练数据集和测试数据集的损失变化不大。 综上所述，该算法在收敛方面表现良好，这在实践中具有吸引力，因为它显着降低了计算成本。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkfigure4.PNG)

------------------------

**Figure 4:** Loss convergence

 接下来，为了研究单个树的最大深度如何影响学习的运行时间，我们在{3，4，5，6，7，7}中改变每个单独树的最大深度，并记录一个提升阶段的运行时间。 如图5（a）所示，我们可以看到随着每棵树的最大深度的增加，运行时间几乎呈线性增加。 这表明我们可以用相对较少的时间来训练一棵相对较深的树，这在实践中非常有吸引力，尤其是在大数据情况下。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworkfigure5.PNG)

---------------------------

**Figure 5:** Scalability Analysis of Secure Federated Tree Boosting System  

最后，我们想研究数据大小对我们提出的系统的可伸缩性的影响。 我们通过功能产品扩展功能集。 如图5（b）和图5（c）所示，我们分别研究了特征编号和样本编号的影响。 如图5（b）和图5（c）所示，为了研究这两个变量的影响，我们在{50，500，1000，5000}范围内更改特征编号，在{5000，  10000，30000}范围内更改样本编号。 我们将单个回归树的最大深度固定为3。我们比较一个提升阶段的运行时间，以研究每个变量如何影响算法的效率。 从结果来看，我们在图5（b）和图5（c）上都得到了相似的观察结果。 结果表明样本和特征编号对运行时间的贡献相同。 此外，我们可以看到，即使有相对大的数据，我们提出的框架也可以很好地扩展。

#### Performance of Completely SecureBoost  

为了研究完全SecureBoost安全性和预测准确性方面的性能，我们的目的是回答以下两个问题：（1）仅基于主动方拥有的功能构建的第一棵树是否学会了足够的信息来掩盖残差实际标签？   （2）与SecureBoost相比，Completely SecureBoost是否会遭受很大的性能损失？

首先，我们研究Completely SecureBoost在安全性方面的性能。 根据“信息泄漏分析”一节中的分析，我们根据叶片的纯度评估信息泄漏。 如定理3所述，我们知道，当Completely SecureBoost的第一棵树很好地适合标签信息时，残差不会显示太多标签信息。 因此，为了验证Completely SecureBoost的安全性，我们必须说明完全安全启动的第一棵树确实很好地掩盖了实际标签。我们在两个真实世界的数据集Credit 1和Credit 2上进行实验。如表2所示，我们比较了第一棵树和第二棵树的平均叶纯度。 特别地，平均叶片纯度是加权平均值，其由$\sum_{i=0}^{k} \frac{n_{i}}{n} p_{i}$计算。 在此，$k$表示合计的叶数。  $p_i$和$n_i$定义为叶片纯度和与叶片$i$相关的实例数。  $n$对应于实例总数。 根据表2，在两个数据集上，平均叶子纯度从第一棵树到第二棵树都显着降低，这证明了Completely SecureBoost在信息保护中的有效性。 此外，在两个数据集上，第二棵树的平均叶纯度都略高于0.6，足以防止标签信息泄露。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworktable2.PNG)

-------------------------------------

**Table 2:** First Tree vs. Second Tree in terms of Leaf Purity

接下来，为了研究Completely SecureBoost在预测准确性方面的性能，我们就第一棵树的性能和总体性能进行了比较，将Completely SecureBoost与SecureBoost进行比较。 我们在数据集Credit 1和Credit 2上进行实验。它们都涉及二进制分类的任务。 因此，我们将常用的accuracy  ，ROC曲线下的面积（AUC）和f1-score  视为评估指标。 所有这三个评估指标越高越好。 结果显示在表3中。可以看出，在几乎所有情况下，Completely SecureBoost的性能都与SecureBoost相同。 我们还将在Completely SecureBoost和SecureBoost之间进行成对的Wilcoxon符号秩检验。 比较结果表明，完全SecureBoost与SecureBoost一样准确，显着性水平为0.05。 完全SecureBoost仍可保证无损属性。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/SecureBoostALosslessFederatedLearningFrameworktable3.PNG)

-----------------------------

**Table 3:** SecureBoost vs. Completely SecureBoost in terms of Classification Performance

###  Conclusion  

在本文中，我们提出了一种新颖的无损隐私保护算法SecureBoost，用于在训练数据对多方保密的情况下训练高质量的树提升模型。 从理论上讲，我们证明了我们提出的框架与非联合梯度树增强算法一样准确，该算法将所有数据天真地放在一个地方。 连同安全性证明一起，我们讨论了使协议完全安全所需的条件。实验结果表明，即使有相对大的数据，我们提出的SecureBoost也可以很好地扩展。

我们认为，联邦学习的研究才刚刚开始。 尽管在本文中，我们展示了如何使Boosted Tree算法适合联邦学习设置，但是在其他机器学习算法上，要以隐私保护和无损方式进行大量工作。 也可以考虑确保上述特性的其他加密算法。

