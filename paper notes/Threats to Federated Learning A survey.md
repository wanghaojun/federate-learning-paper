## Threats to Federated Learning: A survey

作者：Lingjuan Lyu, Han Yu, Qiang Yang

By 李锐

## 摘要

随着数据竖井的出现和隐私意识的普及，传统的集中训练人工智能模型的方法面临着强烈的挑战，联邦学习(FL)则是一个很有前途的解决方案。现有的FL协议设计已经显示出可以被系统内外的对手利用的漏洞，从而危及数据隐私。因此，最重要的是让FL系统设计者意识到未来FL算法设计对隐私保护的影响。在本文中，我们填补了这一重要空白的FL文献。通过简要介绍FL的概念，以及一个涵盖威胁模型和FL的两个主要攻击：1）中毒攻击和2）推理攻击的独特分类法，本文对这一重要主题进行了一次回顾，我们强调了直觉、关键技术以及各种攻击所采用的基本假设，并讨论了未来在FL中更健壮的隐私保护方面的研究方向。

## 1 引言

### 1.1 联邦学习种类

**横向联邦学习（HFL）：**主要分为两类，针对企业的HFL（H2B）和针对消费者的HFL（H2C）。H2B中只有少数参与者，可以在训练期间经常被选中，他们往往拥有强大的计算能力和技术能力。H2C中有数千甚至数百万的潜在参与者，参与者被重复选中培训的机会较低，他们通常拥有有限的计算能力和低技术能力。

**纵向联邦学习（VFL）：**主要针对商业参与者，特征与H2B参与者相似。

**联邦迁移学习（FTL）：**目前发表对FTL模型的威胁的研究较少

### 1.2 FL中的隐私泄露

FL不可能总是提供足够的隐私保障，在整个培训过程中，交流模型更新仍可能泄露敏感信息，甚至深度泄露，要么泄露给第三方，要么泄露给中央服务器。即使是原始梯度的一小部分，也可能揭示出局部数据的信息。NeurIPS 2019的一项最新工作表明，恶意攻击者可以在几次迭代中完全从梯度窃取训练数据。

FL协议设计可能包含两个弱点(1)(潜在的恶意)服务器，谁可以观察个别更新时间，篡改训练过程和控制参与者的视图的全局参数;以及(2)任何能够观察全局参数并控制其参数上传的参与者。本文关注由内部人员对FL系统发起的两个特定威胁：

- 投毒攻击。试图完全阻止模型被学习，或使模型产生偏差，以产生对对手更有利的推论；

- 针对参与者隐私的推理攻击。

## 2 威胁模型

### 2.1 内部vs外部

内部攻击通常比外部攻击要强。对FL的攻击主要集中在三种内部攻击上：单一攻击，拜占庭攻击，女巫攻击。

### 2.2 半诚实vs恶意的

半诚实环境下，对手在不违背FL协议的情况下了解其他方的私密状态。恶意设置下，一个活跃或恶意的对手可以通过修改、重放或删除消息来随意违背FL协议，还可以进行特别毁灭性的攻击。

### 2.3 训练阶段vs推理阶段

训练阶段的攻击企图学习、影响和破坏模型本身。攻击者可以用数据投毒攻击和模型投毒攻击，还可以对参与者的更新发动一系列推理攻击。

推理阶段的攻击也叫逃逸/探索性攻击，它们只会导致模型产生错误的输出或收集关于模型特征的证据。可分为白盒攻击（完全访问FL模型）和黑盒攻击（仅能查询FL模型）。在FL中，当目标模型被部署为服务时，由服务器维护的模型不仅遭受与一般ML设置相同的逃避攻击，FL中的模型广播步骤使任何恶意客户端都可以访问模型。因此，FL需要努力防御白盒闪避攻击。

## 3 投毒攻击

投毒攻击可以分为随机攻击和有针对性的攻击。随机攻击的目的是降低FL模型的准确性，有针对性攻击的目的是诱导FL模型输出对手指定的目标标签。而且有针对性的攻击要比随机攻击更难。下图显示了中毒的更新来自两种投毒攻击：本地数据采集时的数据投毒攻击；在局部模型训练过程中的模型投毒攻击。如果对手能够破坏FL服务器，那么他们可以很容易地对训练好的模型执行有目标和无目标的中毒攻击。

![](https://github.com/Ifendifr/Mypicture/blob/main/1.jpg?raw=true)

### 3.1 数据投毒

数据投毒攻击可以分为clean-label和dirty-label攻击。Clean-label攻击假设对手无法改变任何训练数据的标签，因为存在一个数据被证明属于正确类别的过程，而且数据样本的毒害必须是不可察觉的。相比之下，在脏标签毒害中，敌手可以将一些它希望用所需目标标签误分类的数据样本引入到训练集中。

dirty-label中毒的一个常见例子是标签翻转攻击，将一个类的诚实训练样本的标签翻转到另一个类，同时保持数据的特征不变。另一种微弱但现实的攻击场景是后门中毒。敌手可以修改原始训练数据集的单个特征或小区域，将后门嵌入到模型中，如果输入包含后门特征，模型就会根据敌人的目的表现。但是中毒模型在干净输入上的性能不会受到影响，所以很难检测。

数据中毒攻击可以由任何FL参与者进行。对FL模型的影响取决于系统中参与攻击的参与者的程度，以及被毒害的训练数据的数量。

### 3.2 模型投毒

模型中毒攻击的目标是在将本地模型更新发送到服务器之前进行投毒，或者在全局模型中插入隐藏的后门。

在针对性模型投毒中，敌手的目标使FL模型误分类一组高置信度的选择输入。这些输入在测试时没有被修改来导致错误分类，反而误分类是训练过程中对抗性操作的结果。在任何给定的迭代中，发送到服务器的更新的子集被投毒。这些有毒的更新可以通过插入隐藏的后门而产生，甚至一次单发攻击就足以将后门引入模型中。

通过分析，可以证实在FL设置中模型投毒攻击比数据投毒攻击更有效。为了提高攻击的隐蔽性和规避检测能力，敌手采用交替最小化策略对训练损失和对抗目标进行交替优化，并使用参数估计对良性参与者进行更新。这种对抗性模型中毒攻击可导致FL模型的靶向中毒而不被发现。

## 4 推理攻击

在FL训练期间交换梯度会导致严重的隐私泄露。如图3所示，由于深度学习模型似乎在内部识别了许多与主要任务没有明显关联的数据特征，因此，模型更新可能会将参与者训练数据的非预期特征的额外信息泄露给敌对的参与者。对手也可以保存FL模型参数的快照，并通过利用连续快照之间的差异来进行属性推断，这等于所有参与者减去对手的聚合更新（图4）。

![](https://github.com/Ifendifr/Mypicture/blob/main/2.jpg?raw=true)

![](https://github.com/Ifendifr/Mypicture/blob/main/3.jpg?raw=true)

主要原因是梯度来自于参与者的私有数据。在深度学习模型中，给定一层的梯度是使用这一层的特征和来自上层的误差来计算的。例如卷积层，权值的梯度是上层的误差和特征的卷积。所以对模型更新的观察可以用来推断大量的私有信息，如类代表、成员关系以及与训练数据子集相关的属性。更可怕的是，攻击者可以从共享的梯度中推断出标签，并且不需要任何关于训练集的先验知识就可以恢复原始的训练样本。

### 4.1 推理类代表

Hitaj等人针对深度FL模型设计了一种主动推理攻击，称为生成对抗性网络（GAN）攻击。恶意的参与者可以故意损害任何其他参与者。GAN攻击利用FL学习过程的实时性，使得敌方能够训练一个GAN，生成目标训练数据的原型样本，意味着这些样本是私有的。GAN攻击不是针对重建实际的训练输入，而只针对类的代表。需要注意的是，GAN攻击假设给定类的整个训练语料库来自单个参与者，只有在所有类成员都相似的特殊情况下，GAN构造的代表才与训练数据相似。然而，这些假设在FL中可能不太实际。而且GAN攻击不适合H2C场景，因为它需要大量的计算资源。

### 4.2 推理身份

当给定一个精确的数据点，成员推理攻击的目的是确定它是否被用于训练模型。在FL中，敌手的目标是推断一个特定样本是否属于单一方（如果目标更新是单一方）或任何一方（如果目标更新是聚合）的私有训练数据。例如，在自然语言文本上训练的深度学习模型的嵌入层的非零梯度显示了诚实的参与者在FL模型训练中使用的训练批次中出现的单词。这使得对手能够推断给定的文本是否出现在训练数据集中。

FL系统中的攻击者可以进行主动成员推理攻击和被动成员推理攻击。在被动的情况下，攻击者只观察更新后的模型参数，在不改变局部或全局协同训练过程中进行推理。而在主动情况下，攻击者可以篡改FL模型训练协议，对其他的参与者进行更强的攻击。也就是说，攻击者共享恶意更新，并迫使FL模型共享更多关于攻击者感兴趣的参与者的本地数据的信息。这种攻击被称为梯度上升攻击。

### 4.3 推理属性

敌方可以发起被动和主动的属性推理攻击来推断其他参与者的训练数据的属性，这些属性独立于FL模型类的特征。属性推理攻击假设对手有辅助训练数据，并正确标记了他想要推理的属性。被动的敌手只能通过训练二进制属性分类器来观察/窃听更新并进行推理。主动的敌手可以利用多任务学习来欺骗FL模型，使其能够更好地分离有无属性的数据，从而提取更多的信息。一个敌对的参与者甚至可以推断出某个属性在数据中出现或消失的时间。属性推理中的假设会妨碍其在H2C中的适用性。

### 4.4 推理训练的输入和标签

一种名为“梯度深度泄露”的算法只需几次迭代即可获得训练输入和标签，它可以精确地恢复像素级的原始图像和令牌式匹配的原始文本。赵等人又提出了一种名为改进梯度深泄漏（iDLG）的分析方法，该方法可以利用标签与相应梯度的符号之间的关系，从共享梯度中提取标签。iDLG对于任何一个单热标签交叉熵损失训练的可微模型都是有效的。

推理攻击通常假设对手拥有复杂的技术能力和大量的计算资源。因为对手必须进行多轮FL训练，所以它不适合H2C场景，而更适合H2B场景。这种攻击也强调了保护FL训练期间共享的梯度的必要性。

## 5 讨论与前景

为了提高FL系统的鲁棒性，仍然有一些需要被解决的潜在弱点。

**维数诅咒：**具有高维参数向量的大型模型特别容易受到隐私和安全攻击。共享模型参数在FL中也不是一个很好的设计，它将模型的所有内部状态都打开给推理攻击，并通过投毒攻击使模型的可塑性最大化。为了解决FL的这些基本缺点，有必要探讨是否共享模型更新。相反，以黑箱方式共享不太敏感的信息或仅共享模型预测可能会导致FL中更健壮的隐私保护。

**VFL的威胁：**在VFL中，可能只有一方拥有给定学习任务的标签。目前还不清楚是否所有的参与者都有攻击FL模型的同等能力，以及对HFL的威胁是否可以在VFL上发挥作用。目前大多数威胁仍然集中在HFL上，所以对VFL的威胁，对企业很重要，需要继续探索。

**异质体系结构的FL：**共享模型更新通常仅限于同质FL体系结构，也就是说，同一个模型与所有参与者共享。研究如何扩展FL以协同训练异构体系结构的模型，以及现有的攻击和隐私技术能否适应这种范式就变得很重要。

**分散式联邦学习：**目前正在研究系统中不需要单台服务器的分散式FL，他是一个潜在的学习框架，用于不信任任何第三方的企业之间的协作。在该范例中，每一方都以轮询的方式被选为服务器，但是它可能会遭受新的攻击。最后被选为服务器的一方如果选择插入后门，则更有可能有效地破坏整个模型。

**当前防御的弱点：**具有安全聚合的FL特别容易受到投毒攻击。目前还不清楚对抗性训练是否适用于FL，因为对抗性训练主要是为IID数据开发的，而且它在非IID设置中如何执行仍未可知。此外，对抗性训练通常需要许多时期，这在H2C中是不切实际的的。另一种可能的防御是基于差分隐私。Record-level DP限制了身份推理攻击，但不能阻止属性推理。而Participant-level DP面向数千名用户进行培训，FL模型无法与少数参与者收敛，不适合H2B场景。此外，DP会损害学习模型的准确性，这对行业没有吸引力。需要进一步调查Participant-level DP是否可以保护只有少数参与者的FL系统。

**优化防御机制部署：**当部署防御机制时，FL服务器将需要额外的计算费用。此外，不同的防御机制对不同的攻击可能会有不同的效果，产生不同的代价。所以研究如何优化部署防御机制的时机和公布威慑措施的时机十分重要。

联邦学习仍处于起步阶段，在可预见的未来仍将是一个活跃而重要的研究领域。随着FL的发展，攻击机制也将不断发展。提供一个目前对FL攻击的广泛概述，以便使未来FL系统设计者意识到他们的设计中潜在的弱点。开发一种通用防御机制的最终目标是在不降低模型性能的情况下抵抗各种攻击，这需要来自更广泛的研究团体的跨学科努力。

