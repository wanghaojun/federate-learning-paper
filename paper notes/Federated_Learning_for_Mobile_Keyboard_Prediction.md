**Federated Learning for Mobile Keyboard Prediction**

Andrew Hard, Kanishka Rao, Rajiv Mathews, Franc¸oise Beaufays Sean Augenstein, Hubert Eichner, Chloe Kiddon, Daniel Ramage   

​                                                                                                                                                                                           **by Liu Yangchen**

[toc]

### 摘要

我们使用称为联邦学习的分布式设备上学习框架训练递归神经网络语言模型，以在智能手机的虚拟键盘中预测下一单词。 使用随机梯度下降的基于服务器的训练与使用FederatedAveraging算法在客户端设备上的训练进行了比较。 联邦算法可以针对此用例在更高质量的数据集上进行训练，该算法可以实现更好的预测召回率。

这项工作演示了在客户端设备上训练语言模型而不将敏感的用户数据导出到服务器的可行性和好处。 联合学习环境为用户提供了对其数据的更大控制权，并简化了默认情况下通过在大量客户端设备上进行分布式培训和汇总来整合隐私的任务。

**关键词——** 联邦学习，键盘，语言建模，NLP，CIFG

### 1.INTRODUCTION  

Gboard（Google键盘）是一种用于触摸屏移动设备的虚拟键盘，截止到2018年，它支持400多种语言，安装量超过10亿。除了解码来自输入模式（包括敲击和单词手势键入）的嘈杂信号之外，Gboard还提供 自动更正，字词补全和下一字词预测功能。

随着用户越来越多地转向移动设备，可靠和快速的移动输入方法变得越来越重要。 下一个单词的预测提供了一个方便文本输入的工具。 根据少量的用户生成的先前文本，语言模型（LM）可以预测最可能的下一个单词或短语。 图1提供了一个示例：给定文本“I love you”，Gboard预测用户接下来可能会键入“ and”，“ too”或“ so many”。 建议条中的中心位置保留给最高概率的候选项，而第二和第三最可能的候选项分别占据左侧和右侧位置。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/FederatedLearningforMobileKeyboardPredictionfigure1.PNG)

-------------------------------------

**图1**. Gboard中的下一个单词预测。 基于上下文“I love you”，键盘会预测“and”，“too”和“so many”。

在进行这项工作之前，使用词n-gram有限状态传感器（FST）生成了预测[2]。 参考文献中描述了Gboard中FST解码器的机制-包括FST在文字解码，更正和完成中的作用。  [3]。 通过搜索与前面的文本匹配的最高阶n-gram状态来建立下一个单词预测。 返回此状态的n个最佳输出标签。 还考虑了包含后退过渡到低阶的路径。  Gboard中英语的主要（静态）语言模型是Katz平滑贝叶斯插值[4] 5-gram LM，包含125万个n-gram，包括164,000 ug。 个性化的用户历史记录，联系人和电子邮件n-gram模型增强了主要LM。

移动键盘模型受到多种限制。 为了在低端和高端设备上都可以运行，模型应较小，推理时间延迟应较低。 用户通常期望在输入事件发生20毫秒内看到可见的键盘响应。 考虑到使用移动键盘应用程序的频率，如果不限制CPU消耗，客户端设备的电池可能会很快耗尽。 结果，语言模型通常限于数十兆字节的大小，并且具有成千上万个单词的词汇量。

神经模型，尤其是单词和字符级的递归神经网络（RNN）[5]，在语言建模任务中表现良好[6、7、8]。 与依赖于固定历史上下文窗口的n元语法模型和前馈神经网络不同，RNN使用任意且动态大小的上下文窗口。 使用长短期记忆（LSTM）[6]解决了通过时间反向传播算法中的爆炸和消失梯度。 在撰写本文时，已经使用LSTM变体[10，11]实现了10亿个单词基准[9]上的最新困惑。

训练预测模型需要大量数据样本，该样本代表用户将提交的文本。 尽管训练分布经常与人口分布不匹配，但可以使用公开可用的数据集。另一个选择是对用户生成的文本进行采样。 这需要日志记录，基础结构，服务器上的专用存储和安全性。 即使有了数据清理协议和严格的访问控制，用户也可能不满意其个人数据的收集和远程存储[12]。

在本文中，我们证明了联合学习在商业环境中提供了基于服务器的数据收集和培训范例的替代方法。 我们从头开始在服务器和联合环境中训练RNN模型，并实现相对于FST解码器基准的召回率改进。

本文的组织方式如下。 第2部分总结了与移动输入解码，使用RNN进行语言建模以及联合学习有关的先前工作。 耦合输入忘记门（CIFG）（用于下一词预测的RNN变体）在第3节中进行了介绍。第4节更深入地讨论了联合平均算法。第5节总结了模型的联合和基于服务器的训练的实验。 研究的结果在第6节中介绍，然后在第7节中作总结。

### 2. RELATED WORK  

在移动键盘输入解码，校正和预测的背景下已经探索了FST [3]。  LSTM极大地改善了移动键盘上手势输入的解码[13]。 已针对推理时间延迟和内存限制内的单词预测率和按键节省进行了优化的RNN语言模型[14，15]。

对神经模型的分布式训练的研究与最近对隐私和政府法规的关注日益相关。 特别是，联邦学习已被证明是使用本地存储的数据将基于服务器的分布式培训扩展到基于客户端设备的培训的有用扩展[12、16]。 语言模型已经使用联合差分差异性[17，18]的联合算法进行了训练。 而且，Gboard先前曾使用联合学习来训练模型，以根据键入上下文建议搜索查询[19]，尽管结果尚未发布。 据我们所知，尚无现有出版物为具有联合学习功能的移动键盘训练神经语言模型。

### 3. MODEL ARCHITECTURE  

下一个单词的预测模型使用长短期记忆（LSTM）[6]递归神经网络的一种变体，称为耦合输入和忘记门（CIFG）[20]。 与门控循环单元[21]一样，CIFG使用单个门来控制输入单元和循环单元的自连接，从而将每个单元的参数数量减少了25％。 对于时间步t，输入门t和忘记门ft具有以下关系：
$$
f_i=1-i_t
\tag{1}
$$
CIFG体系结构对移动设备环境有利，因为它减少了计算数量和参数集大小，而对模型性能没有影响。 该模型是使用TensorFlow [22]训练的，没有窥视孔连接。  TensorFlow Lite支持设备上推断。

捆绑的输入嵌入和输出投影矩阵用于减小模型大小并加快训练速度[23，24]。给定一个大小为$v$的词汇表，通过嵌入矩阵$W \in \mathbb{R}^{D \times V}$通过$d = W v$将单热点编码$v \in \mathbb{R}^{V}$映射到密集嵌入向量$d\in \mathbb{R}^{D}$。  CIFG的输出投影（也位于$\mathbb{R}^{D}$中）映射到输出矢量$W^Th \in \mathbb{R}^{V}$。 输出向量上的softmax函数将原始logits转换为归一化的概率。 输出和目标标签上的交叉熵损失用于训练。

第1节中提到的客户端设备要求限制了词汇表和模型的大小。  V = 10,000个单词的词典用于输入和输出词汇。 输入令牌包括特殊的句子开头，句子结尾和语音以外的令牌。 在网络评估和推断期间，将忽略与这些特殊标记相对应的logit。 输入嵌入和CIFG输出投影尺寸D设置为96。使用具有670个单位的单层CIFG。 总体而言，有140万个参数组成了网络-其中三分之二以上与嵌入矩阵W相关联。在权重量化之后，交付给Gboard设备的模型的大小为1.4兆字节。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/FederatedLearningforMobileKeyboardPredictionfigure2.PNG)

-------------------------------------

**图2.**参考文献中的联邦学习过程示意图。  [19] ：( A）客户端设备在本地存储的数据上计算SGD更新，（B）服务器聚合客户端更新以构建新的全局模型，（C）将新模型发送回客户端，并重复上述流程。

### 4. FEDERATED LEARNING

联邦学习[12，16]提供了可用于训练神经模型的分散计算策略。称为客户端的移动设备会生成大量可用于培训的个人数据。 客户端无需将数据上传到服务器进行集中培训，而是处理其本地数据并与服务器共享模型更新。服务器将来自大量客户端的权重进行汇总，并合并以创建改进的全局模型。图2提供了该过程的图示。 事实证明，分布式方法可用于不平衡的数据集以及在客户端之间非独立同分布的数据。

服务器上使用了FederatedAveraging算法[12]来组合客户端更新并生成新的全局模型。 在$t$轮训练中，全局模型$w_t$被发送到客户端设备的子集$K$。 在t = 0的特殊情况下，客户端设备从相同的全局模型开始，该全局模型已被随机初始化或已在代理数据上进行了预训练。 参与给定回合的每个客户都有一个包含$n_k$个示例的本地数据集，其中k是参与客户的索引。  $n_k$因设备而异。 对于Gboard中的研究，$n_k$与用户的打字量有关。

每个客户使用一或多个随机梯度下降（SGD）步骤，使用当前模型$w_t$计算其本地数据的平均梯度$g_k$。 对于客户端学习率，本地客户端更新由下式给出：
$$
w_{t}-\epsilon g_{k} \rightarrow w_{t+1}^{k}
\tag{2}
$$
然后，服务器对客户端模型进行加权聚合以获得新的全局模型$w_{t+1}$：
$$
\sum_{k=1}^{K} \frac{n_{k}}{N} w_{t+1}^{k} \rightarrow w_{t+1}
\tag{3}
$$
其中$N=\sum_{k}n_k$本质上，客户端在本地计算SGD更新，然后将其更新到服务器并进行汇总。 调整了超级参数，包括客户端批处理大小，客户端时期数和每轮客户端数量（全局批处理大小），以提高性能。

即使服务器托管的数据是匿名的，分散的设备上计算也比服务器存储提供更少的安全性和隐私风险。 将个人数据保存在客户端设备上，可以使用户对自己的数据进行更直接的物理控制。 每个客户端传达给服务器的模型更新都是临时的，集中的和聚合的。客户端更新永远不会存储在服务器上。 更新在内存中进行处理，并在权重向量中累加后立即丢弃。 遵循数据最小化原则[25]，上传的内容仅限于模型权重。 最后，结果仅用于汇总：通过组合来自许多客户端设备的更新来改进全局模型。 此处讨论的联合学习过程要求用户相信聚合服务器不会仔细检查各个权重。 这仍然比服务器培训更可取，因为服务器永远不会委托用户数据。 正在探索其他技术来放松此要求。 联邦学习先前已被证明可以与隐私保护技术（例如安全聚合[26]和差分隐私[17]）互补。

### 5. EXPERIMENTS  

从随机权重初始化开始，使用联邦学习和基于服务器的随机梯度下降来训练第3节中描述的CIFG语言模型。 两种模型的性能均根据服务器托管的日志数据和客户端保存的数据进行评估。

#### 5.1. Server-based training with logs data  

基于服务器的CIFG下一个单词预测模型的培训依赖于Gboard用户记录的数据，这些用户选择在键入Google应用程序时共享文本摘要。 文本被截断以包含几个单词的简短短语，并且仅偶尔从各个用户处记录摘要。 在培训之前，日志将被匿名化并去除个人身份信息。 此外，摘要仅在以句子记号开头的情况下才用于训练。

对于本研究，日志是从美国Gboard用户的说英语的人群中收集的。 大约75亿个句子用于培训，而测试和评估样本每个包含25,000个句子。数据集中的平均句子长度为4.1个单词。 表1提供了按应用程序类型分类的日志数据的细分。 聊天应用程序生成大多数记录的文本。

学习速率等于${10}^{-3}$且没有权重衰减或动量的异步随机梯度下降用于训练服务器CIFG。 找不到包括Adam [27]和AdaGrad [28]的自适应梯度方法可以改善收敛性。 句子以50批为单位进行处理。网络经过1.5亿个SGD步骤后收敛。 图3显示了网络训练期间CIFG的前1个召回情况，与n-gram基线模型的性能相比。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/FederatedLearningforMobileKeyboardPredictionfigure3.PNG)

---------------------

**图3.**在服务器培训期间，CIFG的Top-1调用是SGD步骤的函数。 显示了对n-gram FST基线模型的召回以进行比较，但是在此研究中未训练FST模型。

|    App type    | Share of data |
| :------------: | :-----------: |
|      Chat      |      60%      |
|   Web input    |      35%      |
| Long form text |      5%       |

-----------------

**表1.**按移动应用程序类型组成的日志数据。

#### 5.2. Federated training with client caches  

用于CIFG下一个单词预测模型的联合训练的数据存储在Gboard客户端设备上的本地缓存中。与日志数据一样，每个客户端缓存都存储属于设备所有者的文本以及解码器生成的预测候选。

客户端设备必须满足许多要求，才有资格参加联合培训。 就硬件要求而言，设备必须至少具有2 GB的可用内存。 此外，仅当客户端正在充电，连接至未计量的网络并且处于空闲状态时，才允许客户端参与。 这些标准是为联合学习的Gboard实施专门选择的，并不是联合学习平台固有的。 在启用Gboard 7.3或更高版本且启用美国英语语言模型的情况下，还要求本研究的客户位于北美地区。

与基于服务器的训练不同，后者通过显式拆分数据来获取训练，测试和评估样本，而通过定义单独的计算任务来获得联合的训练，测试和评估样本。 虽然没有将客户端设备明确划分为三个不同的群体，但是在足够大的客户端群体中，培训和测试或评估任务中客户端重用的可能性很小。 表2显示了按应用程序类型划分的客户端缓存数据的组成。与日志数据一样，客户端缓存也由聊天应用程序控制。 社交媒体应用程序在客户端缓存示例中的存在增加了，而长形式的通信则较少。

| App type  | Share of data |
| :-------: | :-----------: |
|   Chat    |      66%      |
|  Social   |      16%      |
| Web input |      5%       |
|   Other   |      12%      |

-------------------

**表2.**按移动应用程序类型组成的客户端缓存数据。

第4节中描述的FederatedAveraging算法用于汇总分布式客户端SGD更新。 需要完成100到500个客户端更新才能结束Gboard中的每一轮联合培训。 公式3中的服务器更新是通过Momentum优化器使用Nesterov加速梯度[29]，动量超参数为0.9和服务器学习率为1.0来实现的。 发现该技术相对于包括纯SGD在内的替代方法减少了培训时间。 平均而言，每个客户在一个培训时期内处理大约400个例句。 联盟的CIFG经过3000轮培训后趋于一致，在此过程中，150万客户处理了6亿句话。 培训通常需要4-5天。 联合CIFG的top-1召回情况是训练回合的函数，如图4所示。在联合评估任务中也测量了n-gram基线模型的性能，以提供CIFG的比较，尽管解码器是 在这项研究中未受过训练。 通过将存储在设备上的训练缓存中的解码器候选对象与实际用户输入的文本进行比较，可以测量N-gram模型的召回率。

![](https://raw.githubusercontent.com/beichen777/paperimage/main/FederatedLearningforMobileKeyboardPredictionfigure4.PNG)

------------------------

**图4.**联合训练期间，CIFG的Top-1回想与训练回合的函数关系。  n-gram FST基线模型的性能与CIFG一起在客户端缓存上进行了评估，但本研究未对其进行培训。

### 6. RESULTS  

使用召回指标评估每个模型的性能，该指标定义为正确预测数与令牌总数之比。 召回最高可能性的候选人对于Gboard非常重要，因为用户更倾向于阅读和利用中心建议点中的预测。 由于Gboard在建议栏中包含三名候选人，因此前三名的召回率也很有趣。

| Model               | Top-1 recall | Top-3 recall |
| ------------------- | ------------ | ------------ |
| Baseline n-gram     | 13.0%        | 22.1%        |
| Server-trained CIFG | 16.5%        | 27.1%        |
| Federated CIFG      | 16.4%        | 27.0%        |

-----------------------------------

**表3.**根据服务器托管的日志数据评估的服务器和联合CIFG模型与n-gram基线相比的预测召回率。

服务器托管的日志数据和客户端设备拥有的缓存均用于测量预测调用。 尽管每个缓存都包含来自实际用户的数据片段，但据信客户端缓存可以更准确地表示真正的键入数据分布。 与日志不同，缓存数据的长度不会被截断，并且不限于Google拥有的应用中的键盘用法。 因此，在Gboard的情况下，联合学习可以使用更高质量的训练数据。表3总结了根据服务器托管的日志数据测得的召回性能，而表4显示了使用客户端拥有的缓存评估的性能。

对于服务器培训和联合培训，CIFG模型相对于基线n-gram FST模型提高了top-1和top-3召回率。 鉴于n-gram模型使用的词汇量大了一个数量级，并且包含了个性化的组件（例如用户历史记录和联系人LM），因此这些成就令人印象深刻。

结果还表明，联合CIFG的性能优于服务器培训的CIFG。 表4显示，在评估客户端缓存数据时，相对于服务器培训的CIFG，联合CIFG将top-1召回率提高了5％（绝对值为0.8％）。 对服务器托管的日志数据的比较表明，尽管日志不能代表真实的类型分布，但两种模型的召回率是可比较的。 尽管这种比较并不完全是苹果与苹果之间的比较-在每种情况下都使用了不同口味的SGD-结果表明，联合学习为神经网络模型的基于服务器的训练提供了更好的选择。

| Model               | Top-1 recall [%] |
| ------------------- | ---------------- |
| Baseline n-gram     | 12.5 ± 0.2       |
| Server-trained CIFG | 15.0 ± 0.5       |
| Federated CIFG      | 15.8 ± 0.3       |

----------------------

**表4.**在客户端拥有的数据缓存上评估的服务器和联合CIFG模型与n-gram基线相比的预测召回率。

### 7. CONCLUSION 

我们显示，使用联合学习从头开始训练的CIFG语言模型可以胜过键盘下一词预测任务上相同的服务器训练的CIFG模型和基线n-gram模型。 据我们所知，这代表了联合语言建模在商业环境中的首批应用之一。 联合学习通过跨高度分布的计算设备群体进行培训，同时提高语言模型质量，为用户提供了安全性和隐私优势。

### 8. ACKNOWLEDGEMENTS 

作者要感谢Google AI团队的同事提供了联合学习框架并进行了许多有益的讨论。