## Federated machine learning: Concept and applications

作者：Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong

By 李锐

### 摘要

今天的人工智能仍然面临两大挑战。一是在大多数行业，数据以孤立的岛屿形式存在。二是加强数据隐私和安全。我们提出了一个可能的解决方案来应对这些挑战：安全的联合学习。除了Google在2016年首次提出的联邦学习框架之外，我们还引入了一个全面的安全的联邦学习框架，包括水平联合学习、垂直联合学习和联邦迁移学习。我们提供联邦学习框架的定义、体系结构和应用程序，并提供关于这个主题的现有工作的全面综述。此外，我们建议在组织间建立基于联邦机制的数据网络，作为一种有效的解决方案，在不损害用户隐私的情况下允许知识共享。

### 1 引言

2016年是人工智能成熟的一年。随着AlphaGo击败了人类顶级围棋手，我们真正见证了人工智能的巨大潜力，也开始期待更复杂、更尖端的人工智能技术在无人驾驶汽车、医疗、金融等许多领域的应用。今天，人工智能技术在几乎每个行业和各行各业都显示出其优势。然而，当我们回首AI的发展历程时，不可避免的经历了几次兴衰。人工智能还会出现下一轮低迷吗？什么时候会出现？因为什么因素？目前公众对人工智能的兴趣在一定程度上是由大数据的可用性驱动的：2016年，AlphaGo使用了总计30万场比赛作为训练数据，从而取得了优异的成绩。

随着AlphaGo的成功，人们自然希望像AlphaGo这样的大数据驱动的人工智能能够尽快在我们生活的各个方面实现。然而，现实世界的情况有些令人失望：除了少数行业，大多数领域只有有限的数据或差数据质量，使得AI技术的实现比我们想象的要困难。有没有可能通过跨组织传输数据，将数据融合到一个公共站点中？事实上，在许多情况下，打破数据源之间的障碍即使不是不可能，也是非常困难的。一般来说，任何AI项目所需的数据都涉及多种类型。例如，在一个人工智能驱动的产品推荐服务中，产品销售者拥有关于产品的信息、用户的购买数据，但没有描述用户购买能力和支付习惯的数据。在大多数行业中，数据以孤岛的形式存在。由于行业竞争、隐私安全、行政程序复杂等原因，即使是同一公司不同部门之间的数据整合也面临着重重阻力。几乎不可能整合分散在国家和机构的数据，或者成本被禁止。

与此同时，随着大公司在数据安全和用户隐私方面做出让步的意识日益增强，对数据隐私和安全的重视已成为一个世界性的重大问题。关于公共数据泄露的新闻引起了公共媒体和政府的极大关注。例如，最近Facebook的数据泄露已经引起了广泛的抗议。作为回应，世界各国正在加强保护数据安全和隐私的法律。例如，欧盟于2018年5月25日实施的《通用数据保护条例》(GDPR)[19]。GDPR的目的是为了保护用户的个人隐私和数据安全。它要求企业在用户协议中使用清晰明了的语言，并授予用户“被遗忘权”，即用户可以删除或撤回个人数据。违反该法案的公司将面临高额罚款。美国和中国也在实施类似的保护隐私和安全的行为。例如，中国网络安全法律和民法的一般原则,制定了2017年，要求互联网企业不得泄漏或篡改的个人信息收集,与第三方进行交易数据时,需要确保该合同遵循法律数据保护的义务。这些规定的建立显然将有助于建立一个更文明的社会，但也将对目前人工智能中普遍使用的数据交易程序提出新的挑战。

AI中传统的数据处理模型通常是简单的数据事务模型，由一方收集数据并向另一方传输数据，另一方负责数据的清理和融合。最后，第三方将获取集成的数据并建立模型供其他方使用。模型通常是作为服务出售的最终产品。这一传统的程序面临着上述新的数据法规和法律的挑战。此外，由于用户可能不清楚这些模型的未来用途，这些交易违反了GDPR等法律。因此，我们面临的困境是，我们的数据处于孤岛的状态，但很多情况下，我们被禁止收集、融合和使用数据到不同的地方进行AI处理。如何合法地解决数据碎片化和隔离问题，是当今人工智能研究者和从业者面临的一大挑战。

### 2 联邦学习概述

联邦学习的概念是谷歌最近提出的。他们的主要想法是建立基于数据集的机器学习模型，这些数据集分布在多个设备上，同时防止数据泄漏。最近的改进集中在克服统计的挑战和改善安全联邦学习。也有研究努力使联邦学习更加个性化。以上工作都是针对设备上的联邦学习，其中涉及到分布式移动用户交互，而大规模分布中的通信成本、数据分布的不平衡以及设备可靠性是优化的主要因素。此外，数据是按用户id或设备id分区的，因此在数据空间中是水平的。这一领域的工作与保护隐私的机器学习(如[58])非常相关，因为它也考虑了分散协作学习设置中的数据隐私。为了扩展联邦学习的概念以涵盖组织间的协作学习场景，我们将原来的“联邦学习”扩展为一个通用概念，用于所有保护隐私的分散协作机器学习技术。在[71]中，我们对联邦学习和联邦转移学习技术进行了初步的概述。在本文中，我们进一步调查了相关的安全基础，并探讨了与其他几个相关领域的关系，如多代理理论和隐私保护数据挖掘。在本节中，我们将提供一个更全面的联邦学习定义，其中考虑了数据分区、安全性和应用程序。我们还描述了联邦学习系统的工作流程和系统架构。

#### 2.1 定义

有N个数据所有者$\{F_1,...,F_n\}$，他们都希望通过合并各自的数据来训练机器学习模型$\{D_1,...,D_n\}$。传统的方法是把所有的数据放在一起用$D=D_1\cup...\cup D_n$
并训练模型$M_{sum}$。在联邦学习系统中，数据所有者协同训练一个模型，任何数据所有者都不能将自己的数据暴露给其他人。此外，$M_{FED}$(记为$V_{FED}$)的精度应该非常接近于$M_{sum}$，$V_{sum}$的性能。

如果$|V_{FED}-V_{sum}|<\delta$，其中$\delta$是非负数，就称联邦学习算法有$\delta$-精度损失。

#### 2.2 联邦学习的隐私性

**安全多方计算。**SMC安全模型涉及多方，并在一个定义良好的仿真框架中提供安全证明，以保证完全的零知识，即各方除了其输入和输出之外什么都不知道。在某些场景中，如果提供了安全保证，则可以认为部分知识公开是可接受的。在较低的安全要求下，可以建立具有SMC的安全模型，以换取效率。这些工作要求参与者的数据在互不串通的服务器之间秘密共享。

**差分隐私。**另一项工作是使用差分隐私或k-匿名技术来保护数据隐私。差分隐私、k-匿名、多样化等方法都涉及到对数据添加噪声，或使用泛化方法模糊某些敏感属性，从而使数据无法恢复以保护用户隐私。然而，这些方法仍然需要将数据传输到其他地方。在参考文献中，作者在联邦学习中引入了一种不同的隐私方法，通过在训练期间隐藏客户端的贡献来增加对客户端数据的保护。

**同态加密。**在机器学习过程中，通过加密机制下的参数交换，采用同态加密来保护用户数据的隐私。与差分隐私保护不同，数据和模型本身不会被传输，也不会被另一方的数据猜到。因此，在原始数据级别上几乎不存在泄漏的可能性。

#### 2.3 联邦学习的分类

数据方的特征和样本空间可能不完全相同，根据数据在特征空间和样本ID空间中的分布情况，将联邦学习分为横向联邦学习、纵向联邦学习和联邦迁移学习。  

##### 2.3.1 横向联邦学习

基于样本的联邦学习，被应用到两个数据集的用户特征重叠较多但用户重叠较少的场景中。在谷歌提出一种针对Android手机机型更新的横向联邦学习解决方案的基础上，提出了一种在联合学习框架下保护聚合用户更新隐私的安全聚合方案。

文献[60]中，提出了一种多任务风格的联邦学习系统，允许多个站点在共享知识和保护安全的同时完成单独的任务，还可以解决高通信成本、延迟和容错问题。文献[41]中，作者提议构建一个安全的客户机-服务器结构，允许在客户机设备上构建的模型在服务器站点上协作，模型构建的过程确保了没有数据泄漏。同样，在文献[36]中,基于分布在移动客户端的数据，作者提出改善沟通成本的方法促进集中的培训模型。最近，人们提出了一种名为的深度梯度压缩方法来大大降低大规模分布式训练中的通信带宽。

横向联邦学习可以表示为
$$
X_i=X_j，Y_i=Y_j，I_i\neq I_j，\forall D_i,D_j,i\neq j
$$
安全定义。横向联邦学习系统通常假定诚实的参与者和安全性来对抗诚实但好奇的服务器，只有服务器才能保证数据参与者的隐私。

<img src="https://github.com/Ifendifr/Mypicture/blob/main/4.jpg?raw=true" style="zoom:67%;" />

##### 2.3.2 纵向联邦学习

隐私保护机器学习算法已经被提出用于垂直分割数据，其中包括合作统计分析，关联规则挖掘，安全线性回归，分类和梯度下降。最近提出了一种纵向联邦学习方案来训练一个保护隐私的逻辑回归模型。研究了实体分辨率对学习性能的影响，并将泰勒近似应用于损失函数和梯度函数，从而使同态加密可以用于隐私保护计算。

纵向联邦学习或基于特征的联邦学习适用于两个数据集的用户重叠较多而用户特征重叠较少的情况。纵向联邦学习是将这些不同的特征进行聚合，并以隐私保护的方式计算训练损失和梯度，协同使用双方的数据建立模型的过程。在这样的联邦机制下，各参与方的身份和地位是相同的。可以表示为
$$
X_i\neq X_j,Y_i\neq Y_j,I_i=I_j,\forall D_i,D_j,i\neq j
$$
安全定义。纵向联邦学习系统通常假设诚实但好奇的参与者。例如，在两方情况下，两方是不串通的，他们中的一个最多被一个对手妥协。安全性定义是对手只能从已损坏的客户端获取数据，而不能从其他客户端获取输入和输出之外的数据。为了方便双方之间的安全计算，有时会引入一个半诚实的第三方(STP)，在这种情况下，假定STP不与任何一方串通。在学习结束时，每一方只持有与其自身特征相关联的模型参数，因此在推理时，双方还需要协作产生输出。  

<img src="https://github.com/Ifendifr/Mypicture/blob/main/5.jpg?raw=true" style="zoom:67%;" />

##### 2.3.3 联邦迁移学习

联邦迁移学习适用于两组数据集的用户与用户特征重叠都很少的情况。利用有限的公共样本集学习两个特征空间之间的公共表示，然后应用于只有单侧特征的样本的预测。可以表示为
$$
X_i\neq X_j,Y_i\neq Y_j,I_i\neq I_j,\forall D_i,D_j,i\neq j
$$
安全定义。联邦迁移学习系统通常涉及两方。它的协议类似于垂直联邦学习中的协议。  

<img src="https://github.com/Ifendifr/Mypicture/blob/main/6.jpg?raw=true" style="zoom:67%;" />

#### 2.4 联邦学习系统的架构

##### 2.4.1 横向联邦学习

横向联邦学习系统的典型架构如图所示。在本系统中，具有相同数据结构的k个参与者通过一个参数或云服务器协同学习一个机器学习模型。假设参与者是诚实的，而服务器是诚实的但好奇的，因此不允许任何参与者向服务器泄漏信息。这种系统的培训过程通常包括以下四个步骤：

1. 参与者在本地计算训练梯度，使用加密、差分隐私或秘密共享技术对梯度进行掩码，并将掩码结果发送到服务器
2. 服务器在不了解任何参与者信息的情况下执行安全聚合
3. 服务器将汇总后的结果返回给参与者

参与者用解密后的梯度更新各自的模型。

<img src="https://github.com/Ifendifr/Mypicture/blob/main/7.jpg?raw=true" style="zoom: 50%;" />

迭代通过上述步骤持续到损失函数收敛。这种架构独立于特定的机器学习算法(逻辑回归、DNN等)，所有参与者将共享最终的模型参数。

安全性分析。如果采用或同态加密进行梯度聚合，该结构可以有效地防止半诚实服务器的数据泄漏。但在协作学习过程中，通过恶意参与者训练生成对抗网络，可能会在另一种安全模型中受到攻击。

##### 2.4.2 纵向联邦学习

为了保证培训过程中数据的保密性，第三方协作者C也参与其中。这里我们假设合作者C是诚实的，不与A、B串通，但A、B是诚实的且对彼此好奇。第三方是可信的，因为C方可以被政府等当局扮演，或被安全计算节点取代。联邦学习系统由两部分组成。

第一部分。加密的实体对齐。由于两家公司的用户组不相同，因此系统使用基于加密的用户ID对齐技术来确认双方的共同用户，而无需A和B公开各自的数据。在实体对齐期间，系统不会公开彼此不重叠的用户。

第二部分。加密模型的训练。在确定了公共实体之后，使用这些公共实体的数据来训练机器学习模型。训练过程可分为以下四个步骤：

1. 第三方协作者C创建加密对，向A和B发送公钥
2. A和B对中间结果进行加密和交换，用于梯度和损失计算
3. A和B分别计算加密的梯度并添加额外的掩码，B还要计算加密的损失；A和B将加密值发送给C
4. C解密并将解密后的梯度和损耗返回给A和B；A和B解密梯度，相应地更新模型参数。

<img src="https://github.com/Ifendifr/Mypicture/blob/main/8.jpg?raw=true" style="zoom: 50%;" />

以训练过程使用线性回归和同态加密作为一个例子。为了用梯度下降方法训练线性回归模型，我们需要安全计算其损失和梯度。假设学习率$\eta$，正则化参数$\lambda$，数据集$\{x_i^A\}$，$\{x_i^B,y_i}$和对应于$x_i^A$和$x_i^B$的特征空间的模型参数$ \Theta _A$，$ \Theta _B$。训练的目标是

<img src="https://github.com/Ifendifr/Mypicture/blob/main/9.jpg?raw=true" style="zoom:67%;" />

具体步骤见下图。在模型训练时，A和B的数据在训练中交互不会导致隐私泄露。但是潜在的信息泄露给C可能被认为侵犯隐私，也可能不被认为侵犯隐私。所以A和B可以通过添加加密的随机掩码来进一步隐藏他们的渐变，防止C从A或B那里获取信息。该模型是无损的，同时模型效率取决于加密数据的通信开销和计算开销。在每次迭代中，A和B之间发送的信息随着重叠样本的数量而缩放。因此，采用分布式并行计算技术可以进一步提高算法的效率。

<img src="https://github.com/Ifendifr/Mypicture/blob/main/10.jpg?raw=true" style="zoom: 50%;" />

<img src="https://github.com/Ifendifr/Mypicture/blob/main/11.jpg?raw=true" style="zoom: 50%;" />

安全性分析。表所示的训练协议没有向透露任何信息，因为所学习的都是掩码后的梯度，且掩码矩阵的随机性和保密性得到了保证。在训练过程的最后，每一方（或）都不受另一方数据结构的影响，只获得与自身特征相关的模型参数。在推理时，步骤如表所示，仍然不会导致信息泄露。

##### 2.4.3 联邦迁移学习

在上面描述的架构到目前为止只适用于重叠的数据集。为了将其覆盖范围扩展到整个样本空间，引入了迁移学习。它并没有改变整体架构，而是改变了A和B之间交换的中间结果的细节。迁移学习通常涉及学习A和B方特征之间的共同表示，以及通过利用源域参与方（在本例中为B）中的标签，将预测目标域方的标签的错误最小化。因此，A和B的梯度计算不同于垂直联合学习场景中的梯度计算。在推理时，仍然需要双方计算预测结果。

##### 2.4.4 激励机制

开发一个公平的平台和激励机制可以鼓励更多的组织加入数据联盟。上述架构的实现不仅考虑了多个组织之间的隐私保护和协作建模的有效性，还考虑了如何奖励贡献出更多数据的组织，以及如何以一致的机制实现激励。因此，联邦学习是一种“闭环”学习机制。  

### 3 相关技术

联邦学习作为一种新技术，具有多种创新思路，其中有一些是根植于已有领域的。下面我们从多个角度来解释联邦学习与其他相关概念的关系。

#### 3.1 隐私保护学习

联邦学习可以看作是保护隐私的分散协作机器学习，因此它与多方保护隐私的机器学习有着密切的联系。文献[17,67]提出了用于垂直分区数据的安全多方决策树算法。Vaidya和Clifton提出了用于垂直分区数据的安全关联挖掘规则、安全k-means、朴素贝叶斯分类器。Ref[31]提出了一种用于水平分区数据的关联规则算法。文献[16]为多方线性回归和分类提出了安全协议。文献[68]提出了安全的多方梯度下降方法。

Nikolaenko等人使用同态加密实现了一种隐私保护协议，用于对水平分区数据进行线性回归。的混乱电路和文献提出了一种对垂直分区数据进行线性回归的方法。这些系统直接解决了线性回归问题。文献用随机梯度下降解决了这个问题，他们还提出了逻辑回归和神经网络的隐私保护协议。最近，文献提出了一个三服务器模型的后续工作。等人提出了一种使用同态加密的安全逻辑回归协议。和提出了通过交换更新的参数来训练水平分区数据的神经网络。文献使用了加性同态加密来保护梯度的隐私性，提高了系统的安全性。

#### 3.2 联邦学习和分布式机器学习

横向联邦学习有点类似于分布式机器学习。参数服务器是分布式机器学习中的一个典型元素。它作为加速训练过程的工具，将数据存储在分布的工作节点上，通过一个中央调度节点分配数据和计算资源。对于横向联邦学习，工作节点代表模型训练的数据拥有方，有完全的自治权限。在参数服务器中，中央节点始终占据控制，因此联邦学习面临更复杂的学习环境。其次，联邦学习强调对数据所有者的数据隐私保护。

和分布式机器学习设置一样，联邦学习也需要处理非id数据。在非id本地数据下，联邦学习的性能可以大大降低。针对这一问题，作者提出了一种类似迁移学习的新方法。

#### 3.3 联邦学习和边缘计算

联邦学习可以看作是边缘计算的操作系统，因为它提供了协调和安全的学习协议。在[69]中，作者考虑了使用基于梯度下降的方法训练的一类机器学习模型。他们基于分布梯度下降的收敛界，提出了一种控制算法，可以确定局部更新和全局参数聚合的最佳平衡点，使损失函数最小化。  

#### 3.4 联邦学习和分布式数据库系统

联邦数据库系统通常对数据库单元使用分布式存储，而且每个数据库单元中的数据都是异构的。但是，联邦数据库系统不涉及任何隐私保护机制，所有数据库单元对管理系统是完全可见的。而且联邦数据库系统的重点是数据的基本操作，而联邦学习的目的是在保护数据隐私的前提下，为每个数据所有者建立一个联合模型，使数据所包含的各种价值和规律为我们更好的服务。

### 4 应用

联邦学习可以在不损害数据隐私和安全的前提下，对来自多方的数据进行统一建模，在销售、金融和许多其他行业有着广阔的应用前景，其中，由于知识产权、隐私保护等因素，数据无法直接聚合为训练机器学习模型。

以智能零售为例。智能零售业务涉及的数据特征主要包括用户购买力、用户个人偏好和产品特征。在实际应用中，这三个数据特征很可能分散在三个不同的部门或企业中，而且三方存储的数据通常是异构的。联邦迁移学习是解决这些问题的关键。

在多方借贷的银行领域问题上，我们可以使用纵向联邦学习。智能医疗是另一个将从联邦学习技术的崛起中受益匪浅的领域。

### 5 联邦学习和企业的数据联盟

联邦学习不仅是一种技术标准，更是一种商业模式，它为大数据的应用提供了一种新的范式。当各机构所占用的孤立数据不能产生理想的模型时，联合学习机制使得机构和企业无需数据交换就可以共享一个统一的模型。此外，联合学习可以借助区块链技术的共识机制制定公平的利润分配规则。无论数据的规模有多大，数据拥有者都会有加入数据联盟的动机，并从中获利。我们认为，数据联盟业务模式的建立和联邦学习的技术机制应该一起进行。建立一个可以安全共享数据和知识的社区，并根据每个参与者的贡献公平分配收益。